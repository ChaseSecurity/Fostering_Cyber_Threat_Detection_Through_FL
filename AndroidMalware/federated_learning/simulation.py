# -*- coding: utf-8 -*-
import os
import gc
import numpy as np
import importlib
import argparse
import datetime
from typing import List, Dict, List, Optional, Tuple, Union
from collections import OrderedDict
import json
import random

import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
from torch.nn.utils.rnn import pad_sequence
import flwr as fl
from flwr.common import Metrics
from flwr.server.client_proxy import ClientProxy
from flwr.common import FitRes
from logging import INFO
from flwr.common.logger import log
fl.common.logger.configure(identifier="FlowerLog", filename=datetime.datetime.now().strftime("%m_%d_")+"log.txt")

## Need configuration.
client_num_cpus = 4
no_tokens = 218
embedding_dimensions = 8
learningRate = 1e-4
weightDecay = 0

parser = argparse.ArgumentParser(description='Config.')
parser.add_argument('--gpu', type=float, default=0)
parser.add_argument('--client_num', type=int, default=3)
parser.add_argument('--client_min', type=int, default=3)
parser.add_argument('--client_epoch', type=int, default=2)
parser.add_argument('--rounds', type=int, default=30)
parser.add_argument('--client_path', type=str, default='3quantity_noniid10')
parser.add_argument('--batch_size', type=int, default=2)
parser.add_argument('--agr', type=str, default='FedAvg')
parser.add_argument('--model_path',type=str,default='no')
parser.add_argument('--compromised_num', type=int,default=0)
parser.add_argument('--p', type=float, default=1.0)
parser.add_argument('--beta', type=float, default=0)

args = parser.parse_args()
client_num_gpus = args.gpu
client_num = args.client_num
client_min = args.client_min
ratio = args.client_min/args.client_num
client_epoch = args.client_epoch
rounds = args.rounds
client_path = args.client_path
batch_size = args.batch_size
client_epoch = args.client_epoch
agr=args.agr
model_path = args.model_path

compromised_num = args.compromised_num
p = args.p
beta = args.beta # For Trimmed Mean

#  Load the AGR
AGR = getattr(importlib.import_module(f"flwr.server.strategy.{agr.lower()}"), agr)

# Randomize the compromised clients.
a = range(1, client_num+1)
random.seed(42)
compromised = random.sample(a,compromised_num)

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# DEVICE = torch.device("cpu")

# Define the model.
class damd_cnn(nn.Module):
    def __init__(self, no_tokens):
        super().__init__()
        embedding_dimensions = 8
        no_convolutional_filters = 64
        kernel_size = embedding_dimensions
        nOutputSamples = no_convolutional_filters
        nClasses = 2
        nHidden = 16
        self.embed = nn.Embedding(num_embeddings=no_tokens+1, embedding_dim=embedding_dimensions)
        self.embed.weight.requires_grad = False
        self.conv = nn.Conv1d(in_channels=embedding_dimensions, out_channels=no_convolutional_filters, kernel_size=kernel_size, stride=1, padding='valid')
        self.relu1 = nn.ReLU()
        self.max = nn.AdaptiveMaxPool1d(1)
        self.line1 = nn.Linear(in_features=nOutputSamples, out_features=nHidden)
        self.relu2 = nn.ReLU()
        self.line2 = nn.Linear(in_features=nHidden, out_features=nClasses)

    def forward(self, xb):
        xb = self.embed(xb)
        xb = xb.permute(0,2,1)
        xb = self.conv(xb)
        xb = self.relu1(xb)
        xb = self.max(xb)
        xb = xb.squeeze(-1)
        xb = self.line1(xb)
        xb = self.relu2(xb)
        xb = self.line2(xb)
        xb = torch.softmax(xb, dim=1)
        return xb

# Rewrite TensorDataset.
# Load the apks only when getting items from datasets.
class myDataset(TensorDataset):
    def __init__(self, name_list, data_label, data_size, data_path):
        super().__init__()
        self.name_list = name_list
        self.label = data_label
        self.data_size = data_size
        self.data_path = data_path
    # Index is from dataloader.
    def __getitem__(self, index):
        self.data_filename = self.name_list[index]
        full_path = os.path.join(self.data_path, self.data_filename)
        with open(full_path, 'r') as f:
            indices=np.array(f.read().strip('\n').split(','), dtype=np.uint8)
        labels = self.label[index]     
        return torch.from_numpy(indices).to(torch.int), torch.as_tensor([labels]).to(torch.int)
    # Return size of data for TensorDataset implementation.
    def __len__(self):
        return self.data_size

# Rewrite Dataloader.
# Return opcode sequences of same size.
def collate_fn(batch):
    data_list, label_list = [], []
    for (_data, _label) in batch:
        label_list.append(_label)
        data_list.append(_data)
    # Padding all the sequences to the same size.
    data_list = pad_sequence(data_list, batch_first=True, padding_value=0)
    label_list = torch.as_tensor(label_list).to(torch.long)
    return data_list, label_list


# Get file names and labels for dataset, and not loading all the files.
def get_filenames(data_path, cid):
    fname_list = os.listdir(data_path)
    fname_len = len(fname_list)
    log(INFO, f'Loading {fname_len} filenames into datasets ...')
    labels = []

    if eval(cid) not in compromised or cid == -1:
        for filename in fname_list:
            if filename == '.DS_Store':
                continue
            if filename.split('.')[-1] == '0':
                labels.append(0)
            else:
                labels.append(1)
    else:
        if compromised_num != 0:
            poison_files = random.sample(fname_list, int(fname_len * p))
        for filename in fname_list:
            if filename == '.DS_Store':
                continue
            if filename in poison_files:
                #label flip
                if filename.split('.')[-1] == '0':
                    labels.append(1)
                else:
                    labels.append(0)
            else:
                if filename.split('.')[-1] == '0':
                    labels.append(0)
                else:
                    labels.append(1)

    return fname_list, labels, fname_len

# Calculate losses of a batch.
def loss_batch(network, loss_func, x, y, opt=None):
    # Forward.
    criterion = loss_func().to(DEVICE)
    pred = network(x)
    loss = criterion(pred, y)
    if opt is not None:
        loss.backward()
        opt.step()
        opt.zero_grad()


def test(network, test_dl, loss_func):
    correct, total, loss = 0, 0, 0.000
    TP, TN, FN, FP = 0.000, 0.000, 0.000, 0.000
    network.eval()  # Take all the network parameters
    with torch.no_grad() :
        for x, y in test_dl:
            x = x.to(DEVICE) 
            y = y.to(DEVICE) 
            pred = network(x)
            criterion = loss_func()
            loss += criterion(pred, y)
            total = total + 1

            TP += ((pred.argmax(1) == 1) & (y == 1)).cpu().sum()
            TN += ((pred.argmax(1) == 0) & (y == 0)).cpu().sum()
            FN += ((pred.argmax(1) == 0) & (y == 1)).cpu().sum()
            FP += ((pred.argmax(1) == 1) & (y == 0)).cpu().sum()

            p = TP / (TP + FP)
            r = TP / (TP + FN)
            F1 = 2 * r * p / (r + p)
            acc = (TP + TN) / (TP + TN + FP + FN)
    return loss/total, p, r, F1, acc


def train(network, train_dl, no_epochs, loss_func):
    starttime = datetime.datetime.now()
    opt = optim.Adam(network.parameters(), lr=learningRate, eps=1e-08, weight_decay=weightDecay)
    for epoch in range(no_epochs):
        # Training stage
        network.train()  # Take random network parameters.
        for x, y in train_dl:
            x = x.to(DEVICE)
            y = y.to(DEVICE)
            loss_batch(network, loss_func, x, y, opt)
            gc.collect()

    endtime = datetime.datetime.now()
    client_train_time = (endtime - starttime).seconds
    # if not os.path.isdir('models'):
    #    os.makedirs('models')
    # network.save('models/damd_model_%d' % epoch)


# Define Flower client
test_path = '../data/{}/test'.format(client_path)
filenames_sorted, labels, filelist_len = get_filenames(test_path, str(-1))
dataset = myDataset(filenames_sorted, labels, filelist_len, test_path)
test_dl = DataLoader(dataset=dataset, batch_size=1, shuffle=True, collate_fn=collate_fn, num_workers=0)


def get_parameters(net) -> List[np.ndarray]:
    return [val.cpu().numpy() for _, val in net.state_dict().items()]

def set_parameters(net, parameters: List[np.ndarray]):
    params_dict = zip(net.state_dict().keys(), parameters)
    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
    net.load_state_dict(state_dict, strict=True)

class FlowerClient(fl.client.NumPyClient):
    def __init__(self, cid, net, train_dl) -> None:
        self.cid = cid
        self.net = net
        self.train_dl = train_dl

    def get_parameters(self, config):
        log(INFO, f"[Client {self.cid}] get_parameters")
        return get_parameters(self.net)

    def fit(self, parameters, config):
        log(INFO, f"[Client {self.cid}] fit, config: {config}")
        set_parameters(self.net, parameters)
        train(self.net, self.train_dl, client_epoch, nn.CrossEntropyLoss)
        return get_parameters(self.net), len(self.train_dl), {}

    def evaluate(self, parameters, config):
        log(INFO, f"[Client {self.cid}] evaluate, config: {config}")
        set_parameters(self.net, parameters)
        loss, precision, recall, F1_score, accuracy = test(self.net, test_dl, nn.CrossEntropyLoss)
        return float(loss), len(test_dl), {"accuracy": float(accuracy)}


def client_fn(cid: str) -> FlowerClient:
    """Create a Flower client representing a single organization."""
    # Load model
    net = damd_cnn(no_tokens).to(DEVICE)

    # Load data
    # Note: each client gets a different trainloader/valloader, so each client will train and evaluate on their own unique data.
    destination_folder = '../data/'+client_path
    client_path = '{}/trainer{}'.format(destination_folder, int(cid)+1)
    filenames_sorted, labels, filelist_len = get_filenames(client_path, cid)
    dataset = myDataset(filenames_sorted, labels, filelist_len, client_path)
    # Calculate dataset num.
    train_dl = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)

    # Create a single Flower client representing a single organization
    return FlowerClient(cid, net, train_dl)


class SaveModelStrategy(AGR):
    def aggregate_fit(
        self,
        server_round: int,
        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],
        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],
    ) :
        """Aggregate model weights using weighted average and store checkpoint"""
        net = Net.to(DEVICE)
        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics
        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)
        print(f"Saving round {server_round} aggregated_parameters...")

        if aggregated_parameters is not None:
            print(f"Saving round {server_round} aggregated_parameters...")

            # Convert `Parameters` to `List[np.ndarray]`
            aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)

            # Convert `List[np.ndarray]` to PyTorch`state_dict`
            params_dict = zip(net.state_dict().keys(), aggregated_ndarrays)
            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})
            net.load_state_dict(state_dict, strict=True)

            # Save the model
            if model_path != 'no':
                if not os.path.exists(model_path):
                    os.mkdir(model_path)
                torch.save(net.state_dict(), f"{model_path}/model_round_{server_round}.pth")

        return aggregated_parameters, aggregated_metrics


def evaluate(
        server_round: int, parameters: fl.common.NDArrays, config: Dict[str, fl.common.Scalar]
) -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:
    net = damd_cnn(no_tokens).to(DEVICE)
    set_parameters(net, parameters)  # Update model with the latest parameters

    loss, precision, recall, F1_score, accuracy = test(net, test_dl, nn.CrossEntropyLoss)
    log(INFO,datetime.datetime.now())
    log(INFO, f"Server-side evaluation loss {float(loss)} / precision {float(precision)} / recall {float(recall)} / F1_score {float(F1_score)} / accuracy {float(accuracy)}")
    return float(loss), {"precision":float(precision), "recall":float(recall), "F1_score":float(F1_score), "accuracy": float(accuracy)}


# Define metric aggregation function
def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:
    # Multiply accuracy of each client by number of examples used
    accuracies = [num_examples * m["accuracy"] for num_examples, m in metrics]
    examples = [num_examples for num_examples, _ in metrics]

    # Aggregate and return custom metric (weighted average)
    return {"accuracy": sum(accuracies) / sum(examples)}


# Define strategy
strategy_params = {
    'fraction_fit': ratio,
    'fraction_evaluate': 0,
    'min_fit_clients': client_min,
    'min_evaluate_clients': 0,
    'min_available_clients': client_num,
    'evaluate_fn': evaluate,
}
if agr == 'Krum':
    strategy_params['num_malicious_clients'] = compromised_num
    strategy_params['num_clients_to_keep'] = client_min - compromised_num
elif agr == 'FedTrimmedAvg':
    strategy_params['beta'] = beta
    
# Change 'AGR' to 'SaveModelStrategy' can save the model
strategy = AGR(**strategy_params)

metrics = fl.simulation.start_simulation(
    client_fn = client_fn,
    num_clients=client_num,
    config=fl.server.ServerConfig(num_rounds=rounds),
    strategy=strategy,
    client_resources={
        "num_cpus": client_num_cpus,
        "num_gpus": client_num_gpus,
    },
    #ray_init_args = {"include_dashboard": True},
)

# Write results to json files.
def write_metrics_json_file(path):
    metrics_name = ['loss','precision','recall','F1_score', 'accuracy']
    if not os.path.exists(path):
        os.mkdir(path)
    # Write losses to json file.
    for metric_name in metrics_name:
        json_path = f"{path}/{client_path}_{datetime.datetime.now().strftime('%m_%d_%H')}_{metric_name}.json"
        with open(json_path,'w') as f:
            if metric_name == 'loss':
                list_name = metrics.losses_centralized
            else:
                list_name = metrics.metrics_centralized[metric_name]
            for data in list_name:
                line = json.dumps(data)
                f.write(line)
                f.write("\n")

write_metrics_json_file('./logs')
