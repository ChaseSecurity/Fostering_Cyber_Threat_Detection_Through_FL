# -*- coding: utf-8 -*-
# test_dataset is the same with fl learning
import os
import numpy as np
import datetime
import argparse
import json

import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
from torch.nn.utils.rnn import pad_sequence
import logging
format_str = '%(asctime)s %(levelname)s %(message)s'
logging.basicConfig(level=logging.INFO, format=format_str, filename=datetime.datetime.now().strftime("%m_%d_")+"central_log.txt")

parser = argparse.ArgumentParser(description='Config.')
parser.add_argument('--token_path', type=str, default='testConverted')
parser.add_argument('--batch_size', type=int, default=5)
parser.add_argument('--validset_size', type=float, default=0.1)
parser.add_argument('--epochs', type=int, default=10)
parser.add_argument('--learningRate', type=float, default=1e-4)

args = parser.parse_args()
token_path = args.token_path
batch_size = args.batch_size
validset_size = args.validset_size
epochs = args.epochs
learningRate = args.learningRate

no_tokens = 218
embedding_dimensions = 8
validset_size = 0.1
weightDecay = 0

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
logging.info(DEVICE)


def write_json_file(path,list_):
    with open(path,'w') as f:
        for i, data in enumerate(list_, start=1):
            if type(data) == 'float':
                line = json.dumps([i,data])
            else:
                line = json.dumps([i,data.item()])
            f.write(line)
            f.write("\n")
    f.close()

# Define the model.
class damd_cnn(nn.Module):
    def __init__(self, no_tokens):
        super().__init__()
        embedding_dimensions = 8
        no_convolutional_filters = 64
        kernel_size = embedding_dimensions
        nOutputSamples = no_convolutional_filters
        nClasses = 2
        nHidden = 16
        self.embed = nn.Embedding(num_embeddings=no_tokens+1, embedding_dim=embedding_dimensions)
        self.embed.weight.requires_grad = False
        self.conv = nn.Conv1d(in_channels=embedding_dimensions, out_channels=no_convolutional_filters, kernel_size=kernel_size, stride=1, padding='valid')
        self.relu1 = nn.ReLU()
        self.max = nn.AdaptiveMaxPool1d(1)
        self.line1 = nn.Linear(in_features=nOutputSamples, out_features=nHidden)
        self.relu2 = nn.ReLU()
        self.line2 = nn.Linear(in_features=nHidden, out_features=nClasses)

    def forward(self, xb):
        xb = self.embed(xb)
        xb = xb.permute(0,2,1)
        xb = self.conv(xb)
        xb = self.relu1(xb)
        xb = self.max(xb)
        xb = xb.squeeze(-1)
        xb = self.line1(xb)
        xb = self.relu2(xb)
        xb = self.line2(xb)
        xb = torch.softmax(xb, dim=1)

        return xb


# Rewrite TensorDataset.
# Load the apks only when getting items from datasets.
class myDataset(TensorDataset):
    def __init__(self, name_list, data_label, data_size, data_path):
        super().__init__()
        self.name_list = name_list
        self.label = data_label
        self.data_size = data_size
        self.data_path = data_path
    # Index is from dataloader.
    def __getitem__(self, index):
        self.data_filename = self.name_list[index]
        full_path = os.path.join(self.data_path, self.data_filename)
        with open(full_path, 'r') as f:
            indices=np.array(f.read().split(','), dtype=np.uint8)
        labels = self.label[index]
        return torch.from_numpy(indices).to(torch.int), torch.as_tensor([labels]).to(torch.int)
    # Return size of data for TensorDataset implementation.
    def __len__(self):
        return self.data_size

# Rewrite Dataloader.
# Return opcode sequences of same size.
def collate_fn(batch):
    data_list, label_list = [], []
    for (_data, _label) in batch:
        label_list.append(_label)
        data_list.append(_data)
    data_list = pad_sequence(data_list, batch_first=True, padding_value=0)
    label_list = torch.as_tensor(label_list).to(torch.long)
    return data_list, label_list

# Get file names and labels for dataset, and not loading all the files.
def get_filenames(data_path):
    fname_list = os.listdir(data_path)
    fname_len = len(fname_list)
    logging.info(f'Loading {fname_len} filenames into datasets ...')

    labels = []
    for filename in fname_list:
        if filename == '.DS_Store':
            continue
        if filename.split('.')[-1] == '0':
            labels.append(0)
        else:
            labels.append(1)
    return fname_list, labels, fname_len

# Calculate losses of a batch.
def loss_batch(network, loss_func, x, y, opt=None):
    # Forward.
    criterion = loss_func().to(DEVICE)
    pred = network(x)
    loss = criterion(pred, y)
    if opt is not None:
        loss.backward()
        opt.step()
        opt.zero_grad()
    batch_correct = 0
    batch_correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    return  loss.item(), batch_correct/len(x)


def test(network, test_dl, loss_func):
    correct, total, loss = 0, 0, 0.000
    TP, TN, FN, FP = 0.000, 0.000, 0.000, 0.000
    network.eval()  # Take all the network parameters
    with torch.no_grad() :
        for x, y in test_dl:
            x = x.to(DEVICE)
            y = y.to(DEVICE)
            pred = network(x)
            criterion = loss_func()
            loss += criterion(pred, y)
            total = total + 1

            TP += ((pred.argmax(1) == 1) & (y == 1)).cpu().sum()
            TN += ((pred.argmax(1) == 0) & (y == 0)).cpu().sum()
            FN += ((pred.argmax(1) == 0) & (y == 1)).cpu().sum()
            FP += ((pred.argmax(1) == 1) & (y == 0)).cpu().sum()

            p = TP / (TP + FP)
            r = TP / (TP + FN)
            F1 = 2 * r * p / (r + p)
            acc = (TP + TN) / (TP + TN + FP + FN)
    return loss/total, p, r, F1, acc


def train_network_batchwise(data_path, network, no_epochs, loss_func):
    #if not os.path.isdir('models'):
    #    os.makedirs('models')
    opt = optim.Adam(network.parameters(), lr=learningRate, eps=1e-08, weight_decay=weightDecay)

    # Train dataset.
    filenames_sorted, labels, filelist_len = get_filenames(data_path)
    train_dataset = myDataset(filenames_sorted, labels, filelist_len, data_path)
    train_dl = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)
    # Valid dataset.
    valid_path = '../data/test_dataset'
    filenames_sorted, labels, filelist_len = get_filenames(valid_path)
    valid_dataset = myDataset(filenames_sorted, labels, filelist_len, valid_path)
    valid_dl = DataLoader(dataset=valid_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn, num_workers=0)


    loss_train, precision_train, recall_train, F1_score_train, acc_train = [], [], [], [], []
    for epoch in range(no_epochs):
        # Training stage
        network.train()
        for x, y in train_dl:
            x = x.to(DEVICE)
            y = y.to(DEVICE)
            loss_batch(network, loss_func, x, y, opt)

        # Validating stage
        valid_loss, precision, recall, F1_score, valid_acc = test(network, valid_dl, loss_func)
        loss_train.append(valid_loss)
        precision_train.append(precision)
        recall_train.append(recall)
        F1_score_train.append(F1_score)
        acc_train.append(valid_acc)

        # writer.add_scalar('train_loss', valid_loss, epoch) # If you want to use tensorboard.
        logging.info(f'epoch {epoch+1} Train accuracy: {valid_acc} \t Train loss: {valid_loss}')

    ''' Testing stage if you need.
    test_path = '../data/test_dataset'
    filenames_sorted, labels, filelist_len = get_filenames(test_path)
    dataset = myDataset(filenames_sorted, labels, filelist_len, test_path)
    test_dl = DataLoader(dataset=dataset, batch_size=1, shuffle=True, collate_fn=collate_fn, num_workers=0)
    
    test_loss, precision, recall, F1_score, test_correct = test(network, test_dl, loss_func)
    
    logging.info(f'Test accuracy: {test_correct} \t Test loss: {test_loss} \t precision: {precision} \t recall: {recall} \t F1_score: {F1_score}')
    # network.save('models/cental_model_%d' % epoch)
    '''
    return loss_train, precision_train, recall_train, F1_score_train, acc_train

logging.info('Start training...')
central_model = damd_cnn(no_tokens).to(DEVICE)
loss, precision, recall, F1_score, acc = train_network_batchwise('../data/'+token_path, central_model, epochs, nn.CrossEntropyLoss)

# Write results to json files.
def write_metrics_json_file(path):
    metrics_name = ['loss','precision','recall','F1_score', 'accuracy']
    list_name = [loss, precision, recall, F1_score, acc]
    if not os.path.exists(path):
        os.mkdir(path)
    # Write losses to json file.
    for i in range(len(metrics_name)):
        json_path = f"{path}/{token_path}_{datetime.datetime.now().strftime('%m_%d_%H')}_{metrics_name[i]}.json"
        with open(json_path,'w') as f:
            for i, data in enumerate(list_name[i], start=1):
                if type(data) == 'float':
                    line = json.dumps([i,data])
                else:
                    line = json.dumps([i,data.item()])
                f.write(line)
                f.write("\n")

write_metrics_json_file('./logs')